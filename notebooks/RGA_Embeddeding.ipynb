{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzMx3z-ocszV"
      },
      "source": [
        "%pip install chromadb\n",
        "%pip install ollama\n",
        "%pip install pypdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chromadb\n",
        "import ollama # Added for the live LLM call\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from pypdf import PdfReader"
      ],
      "metadata": {
        "id": "48jCeHoZf0YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ID of our fine-tuned model on the Hugging Face Hub\n",
        "EMBEDDING_MODEL_ID = \"arvindcreatrix/bge-baes-my-qna-model\"\n",
        "\n",
        "# The specific LLM we want to use for generation via Ollama\n",
        "GENERATOR_MODEL_ID = \"llama2\"\n",
        "\n",
        "# Path to the PDF file you want to ask questions about\n",
        "PDF_PATH = \"/content/sample_explain.pdf\"\n",
        "\n",
        "# ChromaDB settings\n",
        "CHROMA_PATH = \"./rag_chroma_db\"\n",
        "COLLECTION_NAME = \"rag_collection\"\n",
        "\n",
        "# The instruction prefix required by the BGE embedding model\n",
        "INSTRUCTION_PREFIX = \"Represent this sentence for searching relevant passages: \""
      ],
      "metadata": {
        "id": "smJOuxczf58j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends the prompt to a locally running Qwen2 model using Ollama.\n",
        "\n",
        "    Args:\n",
        "        prompt: The complete prompt to be sent to the LLM.\n",
        "\n",
        "    Returns:\n",
        "        The text response from the LLM.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send the prompt to the specified model\n",
        "        response = ollama.chat(\n",
        "            model=GENERATOR_MODEL_ID,\n",
        "            messages=[{'role': 'user', 'content': prompt}]\n",
        "        )\n",
        "        # Extract and return the content from the response\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred while calling the LLM via Ollama: {e}\")\n",
        "        return \"Error: Could not get a response from the language model. Is Ollama running?\"\n"
      ],
      "metadata": {
        "id": "HMPW8nE9f5x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_pdf(file_path: str) -> list[str]:\n",
        "    \"\"\"Loads a PDF, extracts text, and splits it into chunks.\"\"\"\n",
        "    print(f\"Loading and splitting document: {file_path}\")\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"PDF file not found at: {file_path}\")\n",
        "\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\\n\".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    print(f\"Document split into {len(chunks)} chunks.\")\n",
        "    return chunks\n",
        "\n",
        "def setup_rag_pipeline(pdf_path: str):\n",
        "    \"\"\"Sets up the entire RAG pipeline: embedding, chunking, and indexing.\"\"\"\n",
        "    print(\"--- Setting up RAG pipeline ---\")\n",
        "\n",
        "    # 1. Load and split the document\n",
        "    chunks = load_and_split_pdf(pdf_path)\n",
        "\n",
        "    # 2. Load the fine-tuned embedding model\n",
        "    print(f\"Loading embedding model: {EMBEDDING_MODEL_ID}\")\n",
        "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_ID)\n",
        "\n",
        "    # 3. Setup ChromaDB\n",
        "    print(f\"Setting up ChromaDB at: {CHROMA_PATH}\")\n",
        "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
        "    collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "\n",
        "    # 4. Embed and index the chunks\n",
        "    print(f\"Embedding and indexing {len(chunks)} chunks...\")\n",
        "    chunk_ids = [str(i) for i in range(len(chunks))]\n",
        "    collection.add(\n",
        "        ids=chunk_ids,\n",
        "        documents=chunks,\n",
        "        embeddings=embedding_model.encode(chunks).tolist()\n",
        "    )\n",
        "\n",
        "    print(\"--- RAG pipeline setup complete! ---\")\n",
        "    return collection, embedding_model\n",
        "\n",
        "def ask_question(question: str, collection, embedding_model):\n",
        "    \"\"\"Handles the retrieval and generation for a single question.\"\"\"\n",
        "\n",
        "    # 1. Embed the user's question\n",
        "    instructed_query = INSTRUCTION_PREFIX + question\n",
        "    query_embedding = embedding_model.encode(instructed_query).tolist()\n",
        "\n",
        "    # 2. Retrieve relevant context from ChromaDB\n",
        "    retrieved_results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=3\n",
        "    )\n",
        "\n",
        "    retrieved_context = \"\\n\\n---\\n\\n\".join(retrieved_results['documents'][0])\n",
        "\n",
        "    # 3. Construct the prompt for the LLM\n",
        "    prompt = f\"\"\"\n",
        "    Context:\n",
        "    {retrieved_context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Based on the context provided, please answer the question. If the context does not contain the answer, state that.\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # 4. Get the response from the LLM\n",
        "    answer = get_llm_response(prompt)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "JmiLj1dOf5jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Create a dummy PDF for demonstration if one doesn't exist\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        print(f\"Creating a dummy PDF at: {PDF_PATH}\")\n",
        "        try:\n",
        "            from fpdf import FPDF\n",
        "            pdf = FPDF()\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", size=12)\n",
        "            pdf.multi_cell(0, 10,\n",
        "                \"The theory of general relativity was proposed by Albert Einstein. It describes gravity as a fundamental property of spacetime. \"\n",
        "                \"Isaac Newton developed the laws of motion and universal gravitation, which were the dominant theories for centuries before Einstein. \"\n",
        "                \"In biology, the mitochondrion is known as the powerhouse of the cell, responsible for generating most of the cell's supply of adenosine triphosphate (ATP).\"\n",
        "            )\n",
        "            pdf.output(PDF_PATH)\n",
        "        except ImportError:\n",
        "            print(\"Please install `fpdf` (pip install fpdf) to create the dummy PDF.\")\n",
        "            exit()\n",
        "\n",
        "    # Setup the RAG pipeline (this will take a moment)\n",
        "    collection, embedding_model = setup_rag_pipeline(PDF_PATH)\n",
        "\n",
        "    # Start the interactive Q&A loop\n",
        "    print(\"\\n--- Ollama and Qwen2-0.5B Ready! ---\")\n",
        "    print(\"Enter your questions. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_question = input(\"\\nYour Question: \")\n",
        "        if user_question.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        answer = ask_question(user_question, collection, embedding_model)\n",
        "        print(\"\\nAnswer:\")\n",
        "        print(answer)\n",
        "\n",
        "    print(\"\\n--- Session Ended ---\")"
      ],
      "metadata": {
        "id": "OhhdUqxhf4P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Create a dummy PDF for demonstration if one doesn't exist\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        print(f\"Creating a dummy PDF at: {PDF_PATH}\")\n",
        "        try:\n",
        "            from fpdf import FPDF\n",
        "            pdf = FPDF()\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", size=12)\n",
        "            pdf.multi_cell(0, 10,\n",
        "                \"The theory of general relativity was proposed by Albert Einstein. It describes gravity as a fundamental property of spacetime. \"\n",
        "                \"Isaac Newton developed the laws of motion and universal gravitation, which were the dominant theories for centuries before Einstein. \"\n",
        "                \"In biology, the mitochondrion is known as the powerhouse of the cell, responsible for generating most of the cell's supply of adenosine triphosphate (ATP).\"\n",
        "            )\n",
        "            pdf.output(PDF_PATH)\n",
        "        except ImportError:\n",
        "            print(\"Please install `fpdf` (pip install fpdf) to create the dummy PDF.\")\n",
        "            exit()\n",
        "\n",
        "    # Setup the RAG pipeline (this will take a moment)\n",
        "    collection, embedding_model = setup_rag_pipeline(PDF_PATH)\n",
        "\n",
        "    # Start the interactive Q&A loop\n",
        "    print(\"\\n--- Ollama and Qwen2-0.5B Ready! ---\")\n",
        "    print(\"Enter your questions. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_question = input(\"\\nYour Question: \")\n",
        "        if user_question.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        answer = ask_question(user_question, collection, embedding_model)\n",
        "        print(\"\\nAnswer:\")\n",
        "        print(answer)\n",
        "\n",
        "    print(\"\\n--- Session Ended ---\")"
      ],
      "metadata": {
        "id": "cVwHaCxUgBrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ID of our fine-tuned model on the Hugging Face Hub\n",
        "EMBEDDING_MODEL_ID = \"BAAI/bge-base-en-v1.5\"\n",
        "\n",
        "# The specific LLM we want to use for generation via Ollama\n",
        "GENERATOR_MODEL_ID = \"llama2\"\n",
        "\n",
        "# Path to the PDF file you want to ask questions about\n",
        "PDF_PATH = \"/content/sample_explain.pdf\"\n",
        "\n",
        "# ChromaDB settings\n",
        "CHROMA_PATH = \"./rag_chroma_db\"\n",
        "COLLECTION_NAME = \"rag_collection\"\n",
        "\n",
        "# The instruction prefix required by the BGE embedding model\n",
        "INSTRUCTION_PREFIX = \"Represent this sentence for searching relevant passages: \""
      ],
      "metadata": {
        "id": "bPJbzI6tgBhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Create a dummy PDF for demonstration if one doesn't exist\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        print(f\"Creating a dummy PDF at: {PDF_PATH}\")\n",
        "        try:\n",
        "            from fpdf import FPDF\n",
        "            pdf = FPDF()\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", size=12)\n",
        "            pdf.multi_cell(0, 10,\n",
        "                \"The theory of general relativity was proposed by Albert Einstein. It describes gravity as a fundamental property of spacetime. \"\n",
        "                \"Isaac Newton developed the laws of motion and universal gravitation, which were the dominant theories for centuries before Einstein. \"\n",
        "                \"In biology, the mitochondrion is known as the powerhouse of the cell, responsible for generating most of the cell's supply of adenosine triphosphate (ATP).\"\n",
        "            )\n",
        "            pdf.output(PDF_PATH)\n",
        "        except ImportError:\n",
        "            print(\"Please install `fpdf` (pip install fpdf) to create the dummy PDF.\")\n",
        "            exit()\n",
        "\n",
        "    # Setup the RAG pipeline (this will take a moment)\n",
        "    collection, embedding_model = setup_rag_pipeline(PDF_PATH)\n",
        "\n",
        "    # Start the interactive Q&A loop\n",
        "    print(\"\\n--- Ollama and Qwen2-0.5B Ready! ---\")\n",
        "    print(\"Enter your questions. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_question = input(\"\\nYour Question: \")\n",
        "        if user_question.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        answer = ask_question(user_question, collection, embedding_model)\n",
        "        print(\"\\nAnswer:\")\n",
        "        print(answer)\n",
        "\n",
        "    print(\"\\n--- Session Ended ---\")"
      ],
      "metadata": {
        "id": "oTaswHuegBYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0iKsNIjVgBPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLF-_dn2gBA4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}